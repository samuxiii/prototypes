{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pong.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "uASDXzVGNu7V",
        "colab_type": "code",
        "outputId": "09ff1a43-4341-425e-afd7-57120a6e76ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q keras\n",
        "!pip install gym\n",
        "!pip install â€œgym[atari]\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.11.29)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "/bin/bash: -c: line 0: unexpected EOF while looking for matching `\"'\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "966byywKZ0Br",
        "colab_type": "code",
        "outputId": "26d671c4-0dd4-4fb3-aaef-a2d113fc0790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LkvaAmVKaWDl",
        "colab_type": "code",
        "outputId": "54e60ec3-f501-4947-810b-c8716a4e1762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/job:localhost/replica:0/task:0/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "TaW9fBcvxE0I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.layers import Dense, Conv2D, Flatten, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.memory = []\n",
        "        self.epsilon = 0.009  # exploration rate\n",
        "        self.model = self.__model()\n",
        "\n",
        "    def __model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(16, kernel_size=8, strides=4, activation='relu', input_shape=(80, 80, 1)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv2D(32, kernel_size=4, strides=2, activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dense(3, activation='softmax'))\n",
        "        model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def preprocess(self, I):\n",
        "        # prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector\n",
        "        I = I[35:195]  # crop\n",
        "        I = I[::2, ::2, 0]  # downsample by factor of 2\n",
        "        I[I == 144] = 0  # erase background (background type 1)\n",
        "        I[I == 109] = 0  # erase background (background type 2)\n",
        "        I[I != 0] = 1  # everything else (paddles, ball) just set to 1\n",
        "        I = np.reshape(I, (80, 80, 1))\n",
        "        return I  # shape:(80, 80, 1)\n",
        "\n",
        "    def discount_rewards(self, r, gamma=0.99):\n",
        "        \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "        discounted_r = np.zeros_like(r)\n",
        "        running_add = 0\n",
        "        for t in reversed(range(0, len(r))):\n",
        "            if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "            running_add = running_add * gamma + r[t]\n",
        "            discounted_r[t] = running_add\n",
        "        #normalize\n",
        "        discounted_r -= np.mean(discounted_r)\n",
        "        discounted_r /= np.std(discounted_r)\n",
        "        return discounted_r\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        # states must be preprocessed\n",
        "        if (state.shape[0] != 80 and state.shape[1] != 80):\n",
        "            state = self.preprocess(state)\n",
        "        if (next_state.shape[0] != 80 and next_state.shape[1] != 80):\n",
        "            next_state = self.preprocess(next_state)\n",
        "\n",
        "        # store in memory the different states, actions, rewards...\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self):\n",
        "        # fit model from memory\n",
        "        gamma = 0.5  # importance of the next reward\n",
        "\n",
        "        # initialize\n",
        "        list_x_batch, list_y_batch = [], []\n",
        "        \n",
        "        # get the list of rewards\n",
        "        _,_,list_r_batch,_,_ = zip(*self.memory)\n",
        "        \n",
        "        #print(\"steps:{}\".format(len(self.memory)))\n",
        "        for state, action, reward, next_state, done in self.memory:\n",
        "\n",
        "            state = np.expand_dims(state, axis=0)\n",
        "            \n",
        "            target = np.zeros([3])  #0's for up and down => [0, 0]\n",
        "            target[action] = 1 #performed action is set to 1 \n",
        "\n",
        "            # append\n",
        "            list_x_batch.append(state)\n",
        "            list_y_batch.append(target)\n",
        "                \n",
        "        # clean\n",
        "        self.memory = []\n",
        "\n",
        "        # train the model\n",
        "        x_batch = np.vstack(list_x_batch)\n",
        "        y_batch = np.vstack(list_y_batch)\n",
        "        r_batch = self.discount_rewards(list_r_batch)\n",
        "        #print(r_batch)\n",
        "\n",
        "        self.model.fit(x_batch, y_batch, sample_weight=r_batch, verbose=0)\n",
        "        #self.model.fit(x_batch, yr_batch, verbose=1)\n",
        "\n",
        "    def act(self, state):\n",
        "        # preprocess the sample\n",
        "        state = self.preprocess(state)\n",
        "\n",
        "        if self.epsilon > np.random.rand():\n",
        "            return random.randint(0, 2)\n",
        "\n",
        "        # predict the action to do\n",
        "        state = np.expand_dims(state, axis=0)\n",
        "        action_values = self.model.predict(state)\n",
        "        #print(\"Predictions:{}\".format(action_values))\n",
        "        action = np.argmax(action_values)\n",
        "        '''if action == 1:\n",
        "          print(\"--up\") \n",
        "        elif action == 2:\n",
        "          print(\"--down\")\n",
        "        else:\n",
        "          print(\"--no_action\")\n",
        "          '''\n",
        "        return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DFceIcedNW21",
        "colab_type": "code",
        "outputId": "d409c1fc-fe74-4c9b-bd57-80b219806396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1003
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "#from Agent import Agent\n",
        "from time import sleep\n",
        "\n",
        "#!rm weights.h5\n",
        "\n",
        "# code for the two only actions in Pong\n",
        "UP_ACTION = 2\n",
        "DOWN_ACTION = 3\n",
        "NO_ACTION = 0\n",
        "\n",
        "# initializing our environment\n",
        "env = gym.make(\"Pong-v0\")\n",
        "\n",
        "# beginning of an episode\n",
        "observation = env.reset()\n",
        "\n",
        "# model weights\n",
        "h5file = \"weights.h5\"\n",
        "\n",
        "# agent\n",
        "agent = Agent()\n",
        "\n",
        "# get model\n",
        "if os.path.exists(h5file):\n",
        "    agent.model.load_weights(h5file)\n",
        "\n",
        "# training conf\n",
        "training = True\n",
        "# x_train, y_train, rewards = [], [], []\n",
        "# reward_sum = 0\n",
        "\n",
        "episode = 0\n",
        "previousObs = np.zeros_like(observation)\n",
        "wins=0\n",
        "# main loop\n",
        "while episode < 10000: \n",
        "    # predict action\n",
        "    diffObs = observation - previousObs\n",
        "    \n",
        "    action = agent.act(diffObs)\n",
        "    \n",
        "    #movement = UP_ACTION if action == 1 else DOWN_ACTION\n",
        "    movement = NO_ACTION\n",
        "    if action == 1:\n",
        "      movement = UP_ACTION\n",
        "    elif action == 2:\n",
        "      movement = DOWN_ACTION      \n",
        "\n",
        "    # do one step\n",
        "    next_observation, reward, done, info = env.step(movement)\n",
        "\n",
        "    # save the current observation\n",
        "    agent.remember(diffObs, action, reward, next_observation, done)\n",
        "\n",
        "    # update state\n",
        "    previousObs = observation\n",
        "    observation = next_observation\n",
        "\n",
        "    \n",
        "    if reward != 0:\n",
        "        if reward == 1:\n",
        "            wins += 1\n",
        "\n",
        "        if training:\n",
        "            agent.replay()\n",
        "            agent.model.save_weights(h5file)\n",
        "    \n",
        "    if done:\n",
        "        print(\"******* episode:{} wins:{} (epsilon:{}) ********\".format(episode, wins, agent.epsilon))\n",
        "        \n",
        "        #if wins >= 10:\n",
        "        #  break\n",
        "          \n",
        "        # decrease exploration rate\n",
        "        if agent.epsilon > 0.01:\n",
        "            agent.epsilon *= 0.997\n",
        "        \n",
        "        observation = env.reset()\n",
        "        episode += 1\n",
        "        wins = 0\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******* episode:0 wins:0 (epsilon:0.009) ********\n",
            "******* episode:1 wins:0 (epsilon:0.009) ********\n",
            "******* episode:2 wins:2 (epsilon:0.009) ********\n",
            "******* episode:3 wins:2 (epsilon:0.009) ********\n",
            "******* episode:4 wins:1 (epsilon:0.009) ********\n",
            "******* episode:5 wins:1 (epsilon:0.009) ********\n",
            "******* episode:6 wins:0 (epsilon:0.009) ********\n",
            "******* episode:7 wins:0 (epsilon:0.009) ********\n",
            "******* episode:8 wins:0 (epsilon:0.009) ********\n",
            "******* episode:9 wins:0 (epsilon:0.009) ********\n",
            "******* episode:10 wins:0 (epsilon:0.009) ********\n",
            "******* episode:11 wins:0 (epsilon:0.009) ********\n",
            "******* episode:12 wins:0 (epsilon:0.009) ********\n",
            "******* episode:13 wins:0 (epsilon:0.009) ********\n",
            "******* episode:14 wins:0 (epsilon:0.009) ********\n",
            "******* episode:15 wins:0 (epsilon:0.009) ********\n",
            "******* episode:16 wins:1 (epsilon:0.009) ********\n",
            "******* episode:17 wins:1 (epsilon:0.009) ********\n",
            "******* episode:18 wins:2 (epsilon:0.009) ********\n",
            "******* episode:19 wins:0 (epsilon:0.009) ********\n",
            "******* episode:20 wins:0 (epsilon:0.009) ********\n",
            "******* episode:21 wins:1 (epsilon:0.009) ********\n",
            "******* episode:22 wins:0 (epsilon:0.009) ********\n",
            "******* episode:23 wins:0 (epsilon:0.009) ********\n",
            "******* episode:24 wins:0 (epsilon:0.009) ********\n",
            "******* episode:25 wins:0 (epsilon:0.009) ********\n",
            "******* episode:26 wins:0 (epsilon:0.009) ********\n",
            "******* episode:27 wins:0 (epsilon:0.009) ********\n",
            "******* episode:28 wins:0 (epsilon:0.009) ********\n",
            "******* episode:29 wins:0 (epsilon:0.009) ********\n",
            "******* episode:30 wins:0 (epsilon:0.009) ********\n",
            "******* episode:31 wins:0 (epsilon:0.009) ********\n",
            "******* episode:32 wins:0 (epsilon:0.009) ********\n",
            "******* episode:33 wins:0 (epsilon:0.009) ********\n",
            "******* episode:34 wins:0 (epsilon:0.009) ********\n",
            "******* episode:35 wins:0 (epsilon:0.009) ********\n",
            "******* episode:36 wins:2 (epsilon:0.009) ********\n",
            "******* episode:37 wins:1 (epsilon:0.009) ********\n",
            "******* episode:38 wins:0 (epsilon:0.009) ********\n",
            "******* episode:39 wins:0 (epsilon:0.009) ********\n",
            "******* episode:40 wins:0 (epsilon:0.009) ********\n",
            "******* episode:41 wins:0 (epsilon:0.009) ********\n",
            "******* episode:42 wins:0 (epsilon:0.009) ********\n",
            "******* episode:43 wins:0 (epsilon:0.009) ********\n",
            "******* episode:44 wins:0 (epsilon:0.009) ********\n",
            "******* episode:45 wins:0 (epsilon:0.009) ********\n",
            "******* episode:46 wins:0 (epsilon:0.009) ********\n",
            "******* episode:47 wins:0 (epsilon:0.009) ********\n",
            "******* episode:48 wins:0 (epsilon:0.009) ********\n",
            "******* episode:49 wins:0 (epsilon:0.009) ********\n",
            "******* episode:50 wins:0 (epsilon:0.009) ********\n",
            "******* episode:51 wins:0 (epsilon:0.009) ********\n",
            "******* episode:52 wins:0 (epsilon:0.009) ********\n",
            "******* episode:53 wins:0 (epsilon:0.009) ********\n",
            "******* episode:54 wins:0 (epsilon:0.009) ********\n",
            "******* episode:55 wins:1 (epsilon:0.009) ********\n",
            "******* episode:56 wins:0 (epsilon:0.009) ********\n",
            "******* episode:57 wins:1 (epsilon:0.009) ********\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jngoO1NooILy",
        "colab_type": "code",
        "outputId": "5e87f930-1544-437a-c00d-0e607c613530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "np.random.choice(2, 1, p=(0.6, 0.4))[0]\n",
        "a = np.array([[1, 2], [0, 1], [1,0]])\n",
        "b = np.array([[3], [2], [1]])\n",
        "a*b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 6],\n",
              "       [0, 2],\n",
              "       [1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "-xUqy6gmRMnQ",
        "colab_type": "code",
        "outputId": "7a34ee53-f6bf-408e-eee2-dfd27bc5ec0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "cell_type": "code",
      "source": [
        "!ls weights.h5"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access 'weights.h5': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}